\section{Raft Issues}
\subsection{Tags}
t-config : Configuration issues
t-hang : Hangs
t-CR : Crash Recovery issues
t-LE : Leader Election issues
t-LR : Log Replication issues
t-low : test bugs, debugging, developer logs, software development, minor bugs, documentation, duplicated issue
t-mem-leak : memory leak
t-perf : abnormal cpu-usage
t-lcrugs : local concurrency
t-upgrade : upgrading Logcabin version
\subsection{Issues}
\#1 : t-low\\
\#2 : t-low\\
\#3 : t-low\\
\#4 : t-low\\
\#5 : t-low\\
\#6 : t-low\\
\#7 : t-low\\
\#8 : t-low\\
\#9 : t-low\\
\#10 : t-low\\
\#11 : t-low\\
\#12 : t-low\\
\#13 : t-low\\
\#14 : t-low\\
\#15 : t-low\\
\#16 : t-low\\
\#17 : t-low\\
\#18 : t-low\\
\#19 : t-low\\
\#20 : t-low\\
\#21 : t-low\\
\#22 : t-low\\
\#23 : t-low\\
\#24 : t-low\\
\#25 : t-low\\
\#26 : t-low\\
\#27 : t-low\\
\#28 : t-low\\
\#29 : t-low\\
\#30 : t-low\\
\#31 : t-low\\
\#32 : t-low\\
\#33 : t-low\\
\#34 : t-low\\
\#35 : t-low\\
\#36 : t-low\\
\#37 : t-low\\
\#38 : t-low\\
\#39 : t-low\\
\#40 : t-low\\
\#41 : t-low\\
\#42 : t-low\\
\#43 : t-low\\
\#44 : t-LR\\
\#45 : t-low\\
\#46 : t-low\\
\#47 : t-config\\
\#48 : t-low\\
\#49 : t-perf\\
\#50 : t-hang; hangs in terminating a raft node; related with \#49\\
\#51 : t-low\\
\#52 : t-low\\
\#53 : t-perf\\
\#54 : t-low\\
\#55 : t-low\\
\#56 : t-config; adding new server while existing cluster is running\\
\#57 : t-config; t-perf\\
\#58 : t-low\\
\#59 : t-low\\
\#60 : t-low\\
\#61 : t-low\\
\#62 : t-perf; related with \#30\\
\#63 : t-low\\
\#64 : t-low\\
\#65 : t-low\\
\#66 : t-conf\\
\#67 : t-low\\
\#68 : t-low\\
\#69 : t-low\\
\#70 : t-low\\
\#71 : t-low; similar with \#70\\
\#72 : t-low\\
\#73 : t-low\\
\#74 : t-low\\
\#75 : t-low\\
\#76 : t-low\\
\#77 : t-low\\
\#78 : t-low\\
\#79 : t-low\\
\#80 : t-low\\
\#81 : t-low\\
\#82 : t-low\\
\#83 : t-mem-leak\\
\#84 : t-low\\
\#85 : t-low\\
\#86 : t-low\\
\#87 : t-low\\
\#88 : t-low; merged with \#74\\
\#89 : t-low\\
\#90 : interesting bug\\
The cluster time approximately tracks how long the cluster has been
available with a working leader.\\
Cluster time is measured in nanoseconds and progresses at about the same
rate as a normal clock when the cluster is operational. While there's a
stable leader, the nanoseconds increase according to that leader's
SteadyClock. When a new leader takes over, it starts ticking from cluster
time value it finds in its last entry/snapshot, so some cluster time may be
unaccounted for between the last leader replicating its final entry and then
losing leadership.\\
The StateMachine uses cluster time to expire client sessions. Cluster times
in committed log entries monotonically increase, so the state machine will
see cluster times monotonically increase.\\
Before cluster time, client sessions were expired based on the SystemClock.
That meant that if the SystemClock jumped forwards drastically, all clients
would expire. That's undesirable, so cluster time was introduced in \#90
("make client session timeout use monotonic clock") to address this.\\
Close \#90 (solution): make client session timeout use monotonic clock\\
\#91 : t-low; A race in the unit test: server was replying before request came in.\\
\#92 : t-low\\
\#93 : t-low\\
\#94 : t-config;\\
We just encountered a cluster where clients weren't getting redirected to the leader 
correctly. Followers were redirecting the leader to the wrong node. The cluster 
was oddly configured (the server addresses were not in order), and due to \#47, the 
leader hint addresses were wrong. It would have been nice to spot that right away 
from the ServerStats output.\\
\#95 : t-low\\
\#96 : t-low\\
\#97 : t-LR; t-LE\\
A follower timing out because it's taken too long to write to its own disk. If 
so, can we solve that by simply moving the setElectionTimeout() call in 
handleAppendEntries() lower, past the disk write? Meanwhile the follower holds 
a mutex the entire time, so it won't start an election.\\
\#98 : t-perf\\
\#99 : t-low\\
\#100 : t-low\\
\#101 : t-low\\
\#102 : t-low\\
\#103 : t-low\\
\#104 : t-low\\
\#105 : t-low\\
\#106 : t-low\\
\#107 : t-LE\\
jef : Big Data can trigger LE Timeout >> start LE\\
If the state machine is very far behind and the server is the leader, availability 
suffers. This only really matters for large logs, when log replay time is 
non-negligible. There'd be two parts to fixing this:\\
1. Don't start new elections if your state machine is too far behind your 
commitIndex.\\
At least occasionally persist the commitIndex, so that it doesn't reset all the 
way to 0/lastSnapshotIndex on a reboot.\\
\#108 : t-low\\
\#109 : t-low\\
\#110 : t-low\\
\#111 : t-low\\
\#112 : t-low\\
\#113 : t-low\\
\#114 : t-low\\
\#115 : t-low\\
\#116 : t-low\\
\#117 : t-low\\
\#118 : t-low\\
\#119 : t-low\\
\#120 : t-low\\
\#121 : t-hang; local concurrency : child process deadlock\\
\#122 : t-low; local concurrency detected manually\\
\#123 : t-low\\
\#124 : t-low\\
\#125 : t-upgrade\\
\#126 : t-low\\
\#127 : t-low\\
\#128 : t-low\\
\#129 : t-low\\
\#130 : t-low\\
\#131 : t-low\\
\#132 : t-low\\
\#133 : t-low\\
\#134 : t-low\\
\#135 : t-low\\
\#136 : t-low\\
\#137 : t-low\\
\#138 : t-low\\
\#139 : t-low\\
\#140 : t-low\\
\#141 : t-low\\
\#142 : t-low\\
\#143 : t-low\\
\#144 : t-hang; hang for 1 hour while exiting\\
\#145 : t-low\\
\#146 : t-low\\
\#147 : t-low\\
\#148 : t-low\\
\#149 : t-low\\
\#150 : t-low\\
\#151 : t-low\\
\#152 : t-low\\
\#153 : t-low\\
\#154 : t-low\\
\#155 : t-low\\
\#156 : t-low\\
\#157 : t-low\\
\#158 : t-low\\
\#159 : t-low\\
\#160 : t-LR\\
jeff : go to web to understand the bug\\
\#161 : t-perf; t-LR\\
\#162 : t-low\\
\#163 : t-low\\
\#164 : t-low\\
\#165 : t-perf\\
\#166 : t-low\\
\#167 : t-low\\
\#168 : t-low; related with \#160 and \#161\\
\#169 : t-low\\
\#170 : t-perf; high load cause bug\\
\#171 : t-low\\
\#172 : t-low\\
\#173 : t-lcrugs\\
\#174 : t-LR\\
From looking at the code, I think if a follower reboots while a leader 
is sending it a snapshot, it won't reopen the partially written snapshot 
file. The leader will keep retrying to send an InstallSnapshot request 
with a byte offset in the middle of the file (greater than 1), and the 
follower will keep panicking as a result. I don't see how this would 
resolve itself until the leader steps down or the term changes.\\
\#175 : dangling pointer\\
The issue was that the destructor for ClientImpl calls
ExactlyOnceRPCHelper.exit() which calls into LeaderRPC. Only in case of
MockClientImpl, TreeLeaderRPC needed MockClientImpl to remain live, but
its destructor had already started. This removes the pointer that
TreeLeaderRPC had to MockClientImpl; it now keeps a reference to the
TestingCallbacks directly.
\#176 : t-low\\
\#177 : t-low\\
\#178 : t-low\\
\#179 : t-low\\
\#180 : t-low\\
\#181 : t-low\\
\#182 : t-low\\
\#183 : t-hang; t-LR\\
the issue is that stepDown() changes the configuration 
and then calls interruptAll(), so interruptAll() doesn't 
actually interrupt the recently removed servers.\\
\#184 : t-low\\
\#185 : t-low\\
\#186 : t-low\\
\#187 : t-low\\
\#188 : t-low\\
\#189 : t-low\\
\#190 : t-low\\
\#191 : t-CR\\
Start a LogCabin server and kill it soon. Then restart it. 
Cannot start up with the following error message: 
2015-09-07 02:37:39.719750 Storage/SegmentedLog.cc:999 in 
loadOpenSegment() ERROR[1:evloop]: Segment version read from 
open-19 was 0, but this code can only read version 1 Exiting...\\
It is because in SegmentedLog::prepareNewSegment LogCabin want 
to allocate a 8M file for log entries, and has not set the first 
byte of file to 1 yet. Then we kill it. \\
So in SegmentedLog::loadOpenSegment, LogCabin verify the version 
and found error. But I think it is better just WARNING here, and 
if it is All Zero, the following handling could remove this file. 
Otherwise, this LogCabin server can not start up, and user do not 
know how to do.
\#192 : t-low\\
\#193 : t-low\\
\#194 : t-low\\
\#195 : t-conf; incomplete reset cause two nodes using the same server id\\
\#196 : t-hang\\
\#197 : t-low\\
\#198 : scalability issue; logcabin abort after running out of file descriptors\\
\#199 : t-low\\
\#200 : t-LE\\
If a disk write takes longer than the leader election timeout, a follower 
can call for an election when the leader is still up doing fine.\\
Quick fix suggested by diego is to reset the election timeout at the 
end of handleAppendEntries.
\#201 : continuation from \#200\\
\#202 : related to \#200\\
--- DONE : 12/17/2015 ---